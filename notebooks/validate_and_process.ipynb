{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff230b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c8a7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GitHubDataFetcher:\n",
    "    \"\"\"Fetches and filters data files from GitHub repository.\"\"\"\n",
    "    \n",
    "    def __init__(self, repo_owner: str = \"owlmaps\", repo_name: str = \"map-data\", data_path: str = \"data\"):\n",
    "        \"\"\"\n",
    "        Initialize the fetcher.\n",
    "        \n",
    "        Args:\n",
    "            repo_owner: GitHub repository owner\n",
    "            repo_name: GitHub repository name\n",
    "            data_path: Path to data directory in the repository\n",
    "        \"\"\"\n",
    "        self.repo_owner = repo_owner\n",
    "        self.repo_name = repo_name\n",
    "        self.data_path = data_path\n",
    "        self.api_url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/contents/{data_path}\"\n",
    "        self.raw_base_url = f\"https://raw.githubusercontent.com/{repo_owner}/{repo_name}/master/{data_path}/\"\n",
    "        self.date_pattern = re.compile(r'^(\\d{8})\\.json$')\n",
    "    \n",
    "    def check_accessibility(self) -> bool:\n",
    "        \"\"\"\n",
    "        Check if the GitHub API is accessible.\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if accessible, False otherwise\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 1: Checking GitHub API accessibility...\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(self.api_url, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                print(f\"GitHub API is accessible (Status: {response.status_code})\")\n",
    "                print(f\"  URL: {self.api_url}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"GitHub API returned status code: {response.status_code}\")\n",
    "                print(f\"  Response: {response.text[:200]}\")\n",
    "                return False\n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            print(\"Connection error: Unable to reach GitHub API\")\n",
    "            print(f\"  Error: {str(e)[:200]}\")\n",
    "            return False\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(\"equest timed out after 10 seconds\")\n",
    "            return False\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def fetch_directory_contents(self) -> Optional[List[Dict]]:\n",
    "        \"\"\"\n",
    "        Fetch directory contents from GitHub API.\n",
    "        \n",
    "        Returns:\n",
    "            List of file information dictionaries, or None if failed\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 2: Fetching directory contents...\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(self.api_url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            files_data = response.json()\n",
    "            print(\"Successfully fetched directory contents\")\n",
    "            print(f\"  Total items found: {len(files_data)}\")\n",
    "            \n",
    "            return files_data\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to fetch directory: {e}\")\n",
    "            return None\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Failed to parse JSON response: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def filter_and_parse_files(self, files_data: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Filter JSON files with date pattern and parse their dates.\n",
    "        \n",
    "        Args:\n",
    "            files_data: List of file information from GitHub API\n",
    "            \n",
    "        Returns:\n",
    "            List of parsed file information with dates\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 3: Filtering and parsing JSON files...\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        json_files = []\n",
    "        skipped_files = []\n",
    "        \n",
    "        for item in files_data:\n",
    "            if item.get('type') == 'file':\n",
    "                filename = item.get('name', '')\n",
    "                match = self.date_pattern.match(filename)\n",
    "                \n",
    "                if match:\n",
    "                    date_str = match.group(1)\n",
    "                    try:\n",
    "                        # Parse date to ensure it's valid\n",
    "                        file_date = datetime.strptime(date_str, '%Y%m%d')\n",
    "                        json_files.append({\n",
    "                            'filename': filename,\n",
    "                            'date': file_date,\n",
    "                            'date_str': date_str,\n",
    "                            'download_url': item.get('download_url'),\n",
    "                            'size': item.get('size', 0),\n",
    "                            'sha': item.get('sha', '')\n",
    "                        })\n",
    "                    except ValueError:\n",
    "                        skipped_files.append(filename)\n",
    "        \n",
    "        print(f\"Found {len(json_files)} JSON files with valid date names\")\n",
    "        if skipped_files:\n",
    "            print(f\"Skipped {len(skipped_files)} files with invalid dates:\")\n",
    "            for fname in skipped_files[:5]:\n",
    "                print(f\"    - {fname}\")\n",
    "        \n",
    "        return json_files\n",
    "    \n",
    "    def sort_by_date(self, json_files: List[Dict], descending: bool = True) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Sort files by date.\n",
    "        \n",
    "        Args:\n",
    "            json_files: List of file information with dates\n",
    "            descending: If True, sort newest first; if False, oldest first\n",
    "            \n",
    "        Returns:\n",
    "            Sorted list of files\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 4: Sorting files by date...\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        sorted_files = sorted(json_files, key=lambda x: x['date'], reverse=descending)\n",
    "        \n",
    "        order = \"newest to oldest\" if descending else \"oldest to newest\"\n",
    "        print(f\"Sorted {len(sorted_files)} files ({order})\")\n",
    "        \n",
    "        return sorted_files\n",
    "    \n",
    "    def get_recent_files(self, sorted_files: List[Dict], n: int = 5) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get the N most recent files.\n",
    "        \n",
    "        Args:\n",
    "            sorted_files: List of sorted file information\n",
    "            n: Number of recent files to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            List of N most recent files\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"STEP 5: Selecting {n} most recent files...\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        recent_files = sorted_files[:n]\n",
    "        \n",
    "        print(f\"✓ Selected {len(recent_files)} files:\")\n",
    "        print(f\"\\n{'#':<4} {'Date':<12} {'Filename':<20} {'Size (KB)':<12}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for idx, file_info in enumerate(recent_files, 1):\n",
    "            size_kb = file_info['size'] / 1024\n",
    "            print(f\"{idx:<4} {file_info['date'].strftime('%Y-%m-%d'):<12} {file_info['filename']:<20} {size_kb:>10.1f}\")\n",
    "        \n",
    "        return recent_files\n",
    "    \n",
    "    def display_summary(self, all_files: List[Dict]):\n",
    "        \"\"\"Display summary statistics.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"STEP 6: Summary Statistics\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        if not all_files:\n",
    "            print(\"No files to summarize\")\n",
    "            return\n",
    "        \n",
    "        oldest = all_files[-1]\n",
    "        newest = all_files[0]\n",
    "        total_size_mb = sum(f['size'] for f in all_files) / (1024 * 1024)\n",
    "        avg_size_mb = total_size_mb / len(all_files)\n",
    "        \n",
    "        print(f\"Total JSON files: {len(all_files)}\")\n",
    "        print(f\"Date range: {oldest['date'].strftime('%Y-%m-%d')} to {newest['date'].strftime('%Y-%m-%d')}\")\n",
    "        print(f\"Days covered: {(newest['date'] - oldest['date']).days}\")\n",
    "        print(f\"Total size: {total_size_mb:.2f} MB\")\n",
    "        print(f\"Average file size: {avg_size_mb:.2f} MB\")\n",
    "    \n",
    "    def fetch_and_filter(self, num_recent: int = 5) -> Optional[List[Dict]]:\n",
    "        \"\"\"\n",
    "        Main method to fetch and filter recent files.\n",
    "        \n",
    "        Args:\n",
    "            num_recent: Number of most recent files to return\n",
    "            \n",
    "        Returns:\n",
    "            List of most recent files, or None if failed\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"GITHUB DATA FETCHER - owlmaps/map-data\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Target: {self.repo_owner}/{self.repo_name}/{self.data_path}\")\n",
    "        print(f\"Fetching {num_recent} most recent files...\")\n",
    "        \n",
    "        # Step 1: Check accessibility\n",
    "        if not self.check_accessibility():\n",
    "            return None\n",
    "        \n",
    "        # Step 2: Fetch directory contents\n",
    "        files_data = self.fetch_directory_contents()\n",
    "        if files_data is None:\n",
    "            return None\n",
    "        \n",
    "        # Step 3: Filter and parse files\n",
    "        json_files = self.filter_and_parse_files(files_data)\n",
    "        if not json_files:\n",
    "            print(\"\\nNo valid JSON files found!\")\n",
    "            return None\n",
    "        \n",
    "        # Step 4: Sort by date\n",
    "        sorted_files = self.sort_by_date(json_files, descending=True)\n",
    "        \n",
    "        # Step 5: Get recent files\n",
    "        recent_files = self.get_recent_files(sorted_files, num_recent)\n",
    "        \n",
    "        # Step 6: Display summary\n",
    "        self.display_summary(sorted_files)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"FETCH COMPLETE - Files ready for processing\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        return recent_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b3d432",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataValidator:\n",
    "    \"\"\"Validates JSON data structure and business rules.\"\"\"\n",
    "    \n",
    "    def validate_structure(self, data: dict) -> List[str]:\n",
    "        \"\"\"Validate the basic structure of the data.\"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        # Check required top-level fields\n",
    "        required_fields = ['areas', 'areas_ua', 'frontline', 'geos', 'unit_count', 'units']\n",
    "        for field in required_fields:\n",
    "            if field not in data:\n",
    "                errors.append(f\"Missing required field: {field}\")\n",
    "        \n",
    "        # Validate areas\n",
    "        if 'areas' in data:\n",
    "            if not isinstance(data['areas'], list):\n",
    "                errors.append(\"'areas' must be an array\")\n",
    "            else:\n",
    "                for idx, area in enumerate(data['areas']):\n",
    "                    if not isinstance(area, list):\n",
    "                        errors.append(f\"areas[{idx}] must be an array of coordinates\")\n",
    "                    elif len(area) < 3:\n",
    "                        errors.append(f\"areas[{idx}] must have at least 3 coordinate pairs\")\n",
    "        \n",
    "        # Validate geos\n",
    "        if 'geos' in data:\n",
    "            if not isinstance(data['geos'], dict):\n",
    "                errors.append(\"'geos' must be an object\")\n",
    "            else:\n",
    "                if 'ru' not in data['geos']:\n",
    "                    errors.append(\"'geos' must have 'ru' field\")\n",
    "                if 'ua' not in data['geos']:\n",
    "                    errors.append(\"'geos' must have 'ua' field\")\n",
    "                \n",
    "                for side in ['ru', 'ua']:\n",
    "                    if side in data['geos'] and isinstance(data['geos'][side], list):\n",
    "                        for idx, event in enumerate(data['geos'][side]):\n",
    "                            if not isinstance(event, dict):\n",
    "                                errors.append(f\"geos.{side}[{idx}] must be an object\")\n",
    "                            else:\n",
    "                                if 'c' not in event:\n",
    "                                    errors.append(f\"geos.{side}[{idx}] missing 'c' (coordinates)\")\n",
    "                                if 'd' not in event:\n",
    "                                    errors.append(f\"geos.{side}[{idx}] missing 'd' (description)\")\n",
    "        \n",
    "        # Validate unit_count\n",
    "        if 'unit_count' in data:\n",
    "            if not isinstance(data['unit_count'], dict):\n",
    "                errors.append(\"'unit_count' must be an object\")\n",
    "            else:\n",
    "                for side in ['ru', 'ua']:\n",
    "                    if side not in data['unit_count']:\n",
    "                        errors.append(f\"unit_count.{side} is required\")\n",
    "                    elif not isinstance(data['unit_count'][side], int):\n",
    "                        errors.append(f\"unit_count.{side} must be an integer\")\n",
    "        \n",
    "        # Validate units\n",
    "        if 'units' in data:\n",
    "            if not isinstance(data['units'], dict):\n",
    "                errors.append(\"'units' must be an object\")\n",
    "            else:\n",
    "                for side in ['ru', 'ua']:\n",
    "                    if side not in data['units']:\n",
    "                        errors.append(f\"units.{side} is required\")\n",
    "                    elif not isinstance(data['units'][side], list):\n",
    "                        errors.append(f\"units.{side} must be an array\")\n",
    "        \n",
    "        return errors\n",
    "    \n",
    "    def validate_business_rules(self, data: dict) -> List[Dict]:\n",
    "        \"\"\"Validate additional business rules.\"\"\"\n",
    "        warnings = []\n",
    "        \n",
    "        # Check coordinate ranges\n",
    "        def check_coordinates(coords, context):\n",
    "            if len(coords) >= 2:\n",
    "                lon, lat = coords[0], coords[1]\n",
    "                if not (-180 <= lon <= 180):\n",
    "                    warnings.append({\n",
    "                        'type': 'INVALID_COORDINATE',\n",
    "                        'message': f\"Invalid longitude {lon} in {context}\"\n",
    "                    })\n",
    "                if not (-90 <= lat <= 90):\n",
    "                    warnings.append({\n",
    "                        'type': 'INVALID_COORDINATE',\n",
    "                        'message': f\"Invalid latitude {lat} in {context}\"\n",
    "                    })\n",
    "        \n",
    "        # Validate geos coordinates\n",
    "        if 'geos' in data:\n",
    "            for side in ['ru', 'ua']:\n",
    "                if side in data['geos']:\n",
    "                    for idx, event in enumerate(data['geos'][side]):\n",
    "                        if 'c' in event and isinstance(event['c'], list):\n",
    "                            check_coordinates(event['c'], f\"geos.{side}[{idx}]\")\n",
    "        \n",
    "        # Validate unit coordinates\n",
    "        if 'units' in data:\n",
    "            for side in ['ru', 'ua']:\n",
    "                if side in data['units']:\n",
    "                    for idx, unit in enumerate(data['units'][side]):\n",
    "                        if isinstance(unit, list) and len(unit) >= 2 and isinstance(unit[1], list):\n",
    "                            check_coordinates(unit[1], f\"units.{side}[{idx}]\")\n",
    "        \n",
    "        return warnings\n",
    "    \n",
    "    def validate_file(self, data: dict, filename: str) -> Tuple[bool, List[str], List[Dict]]:\n",
    "        \"\"\"\n",
    "        Validate a single file.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (is_valid, errors, warnings)\n",
    "        \"\"\"\n",
    "        errors = self.validate_structure(data)\n",
    "        warnings = self.validate_business_rules(data)\n",
    "        \n",
    "        # Consider file invalid only if there are structural errors\n",
    "        is_valid = len(errors) == 0\n",
    "        \n",
    "        return is_valid, errors, warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b867b535",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPipeline:\n",
    "    \"\"\"Main pipeline for fetching, validating, and processing data.\"\"\"\n",
    "    \n",
    "    def __init__(self, failed_dir: str = \"../data/failed/\"):\n",
    "        \"\"\"\n",
    "        Initialize the pipeline.\n",
    "        \n",
    "        Args:\n",
    "            failed_dir: Directory to save failed validation files\n",
    "        \"\"\"\n",
    "        self.failed_dir = failed_dir\n",
    "        self.validator = DataValidator()\n",
    "        \n",
    "        # Create failed directory if it doesn't exist\n",
    "        os.makedirs(self.failed_dir, exist_ok=True)\n",
    "    \n",
    "    def download_file_content(self, file_info: Dict) -> Optional[dict]:\n",
    "        \"\"\"\n",
    "        Download and parse JSON file content.\n",
    "        \n",
    "        Args:\n",
    "            file_info: File information with download_url\n",
    "            \n",
    "        Returns:\n",
    "            Parsed JSON data or None if failed\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(file_info['download_url'], timeout=30)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Download error for {file_info['filename']}: {e}\")\n",
    "            return None\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON parse error for {file_info['filename']}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def save_failed_file(self, file_info: Dict, data: dict, errors: List[str], warnings: List[Dict]):\n",
    "        \"\"\"\n",
    "        Save failed validation file with error report.\n",
    "        \n",
    "        Args:\n",
    "            file_info: File information\n",
    "            data: JSON data that failed validation\n",
    "            errors: List of validation errors\n",
    "            warnings: List of validation warnings\n",
    "        \"\"\"\n",
    "        # Save the data file\n",
    "        data_path = os.path.join(self.failed_dir, file_info['filename'])\n",
    "        with open(data_path, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        \n",
    "        # Save error report\n",
    "        report_path = os.path.join(self.failed_dir, f\"{file_info['date_str']}_errors.txt\")\n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(\"VALIDATION FAILURE REPORT\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\")\n",
    "            f.write(f\"Filename: {file_info['filename']}\\n\")\n",
    "            f.write(f\"Date: {file_info['date'].strftime('%Y-%m-%d')}\\n\")\n",
    "            f.write(f\"Size: {file_info['size']} bytes\\n\")\n",
    "            f.write(f\"Download URL: {file_info['download_url']}\\n\")\n",
    "            f.write(f\"Validation Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            f.write(f\"ERRORS ({len(errors)}):\\n\")\n",
    "            f.write(\"-\" * 80 + \"\\n\")\n",
    "            for idx, error in enumerate(errors, 1):\n",
    "                f.write(f\"{idx}. {error}\\n\")\n",
    "            \n",
    "            if warnings:\n",
    "                f.write(f\"\\nWARNINGS ({len(warnings)}):\\n\")\n",
    "                f.write(\"-\" * 80 + \"\\n\")\n",
    "                for idx, warning in enumerate(warnings, 1):\n",
    "                    f.write(f\"{idx}. [{warning['type']}] {warning['message']}\\n\")\n",
    "        \n",
    "        print(f\"  → Saved to failed directory: {file_info['filename']}\")\n",
    "        print(f\"  → Error report: {file_info['date_str']}_errors.txt\")\n",
    "    \n",
    "    def process_files(self, recent_files: List[Dict]) -> Tuple[List[dict], int, int]:\n",
    "        \"\"\"\n",
    "        Process recent files: download, validate, and categorize.\n",
    "        \n",
    "        Args:\n",
    "            recent_files: List of file information from GitHubDataFetcher\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (raw_data list, passed_count, failed_count)\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"DATA VALIDATION AND PROCESSING PIPELINE\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        raw_data = []\n",
    "        passed_count = 0\n",
    "        failed_count = 0\n",
    "        \n",
    "        print(f\"\\nProcessing {len(recent_files)} files...\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for idx, file_info in enumerate(recent_files, 1):\n",
    "            filename = file_info['filename']\n",
    "            print(f\"\\n[{idx}/{len(recent_files)}] Processing: {filename}\")\n",
    "            \n",
    "            # Step 1: Download file content\n",
    "            print(\"  → Downloading...\")\n",
    "            data = self.download_file_content(file_info)\n",
    "            \n",
    "            if data is None:\n",
    "                failed_count += 1\n",
    "                print(\"  FAILED: Could not download or parse file\")\n",
    "                continue\n",
    "            \n",
    "            # Step 2: Validate file\n",
    "            print(\"  → Validating...\")\n",
    "            is_valid, errors, warnings = self.validator.validate_file(data, filename)\n",
    "            \n",
    "            # Step 3: Categorize based on validation\n",
    "            if is_valid:\n",
    "                raw_data.append(data)\n",
    "                passed_count += 1\n",
    "                status = \"PASSED\"\n",
    "                if warnings:\n",
    "                    status += f\" (with {len(warnings)} warnings)\"\n",
    "                print(f\"  {status}\")\n",
    "            else:\n",
    "                self.save_failed_file(file_info, data, errors, warnings)\n",
    "                failed_count += 1\n",
    "                print(f\"  FAILED: {len(errors)} errors found\")\n",
    "        \n",
    "        return raw_data, passed_count, failed_count\n",
    "    \n",
    "    def display_summary(self, raw_data: List[dict], passed_count: int, failed_count: int):\n",
    "        \"\"\"Display processing summary.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"PIPELINE SUMMARY\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        total = passed_count + failed_count\n",
    "        print(f\"\\nTotal files processed: {total}\")\n",
    "        print(f\"  Passed validation: {passed_count} ({passed_count/total*100:.1f}%)\")\n",
    "        print(f\"  Failed validation: {failed_count} ({failed_count/total*100:.1f}%)\")\n",
    "        \n",
    "        if failed_count > 0:\n",
    "            print(f\"\\nFailed files saved to: {self.failed_dir}/\")\n",
    "        \n",
    "        if raw_data:\n",
    "            print(f\"\\nraw_data variable contains {len(raw_data)} validated datasets\")\n",
    "            print(f\"  Type: list of {len(raw_data)} dictionaries\")\n",
    "            print(\"  Each contains the complete JSON data structure\")\n",
    "            \n",
    "            # Quick stats from first dataset\n",
    "            if len(raw_data) > 0:\n",
    "                sample = raw_data[0]\n",
    "                print(\"\\nSample data structure (first file):\")\n",
    "                print(f\"  - areas: {len(sample.get('areas', []))} polygons\")\n",
    "                print(f\"  - frontline: {len(sample.get('frontline', []))} segments\")\n",
    "                print(f\"  - geos.ru: {len(sample.get('geos', {}).get('ru', []))} incidents\")\n",
    "                print(f\"  - geos.ua: {len(sample.get('geos', {}).get('ua', []))} incidents\")\n",
    "                print(f\"  - units.ru: {len(sample.get('units', {}).get('ru', []))} units\")\n",
    "                print(f\"  - units.ua: {len(sample.get('units', {}).get('ua', []))} units\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        if failed_count == 0:\n",
    "            print(\"ALL FILES VALIDATED SUCCESSFULLY\")\n",
    "        else:\n",
    "            print(\"VALIDATION COMPLETE WITH FAILURES\")\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fc46ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"COMPLETE DATA PIPELINE: FETCH → VALIDATE → PROCESS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Step 1: Fetch recent files\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 1: FETCHING RECENT FILES FROM GITHUB\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    fetcher = GitHubDataFetcher()\n",
    "    recent_files = fetcher.fetch_and_filter(num_recent=5)\n",
    "    \n",
    "    if not recent_files:\n",
    "        print(\"\\nFailed to fetch files from GitHub\")\n",
    "        return None, None\n",
    "    \n",
    "    # Step 2: Validate and process files\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 2: VALIDATING AND PROCESSING FILES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    pipeline = DataPipeline(failed_dir=\"../data/failed/\")\n",
    "    raw_data, passed_count, failed_count = pipeline.process_files(recent_files)\n",
    "    \n",
    "    # Step 3: Display summary\n",
    "    pipeline.display_summary(raw_data, passed_count, failed_count)\n",
    "    \n",
    "    # Return the validated data\n",
    "    return raw_data, recent_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f0645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    raw_data, recent_files = main()\n",
    "    \n",
    "    if raw_data:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"READY FOR FURTHER PROCESSING\")\n",
    "        print('='*80)\n",
    "        print(f\"\\nThe 'raw_data' variable contains {len(raw_data)} validated datasets.\")\n",
    "        print(\"You can now process this data further:\")\n",
    "        print(\"  - Analyze trends across dates\")\n",
    "        print(\"  - Extract specific information\")\n",
    "        print(\"  - Generate visualizations\")\n",
    "        print(\"  - Export to other formats\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dev-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
